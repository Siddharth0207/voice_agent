from langchain_nvidia_ai_endpoints import ChatNVIDIA
from langchain.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from dotenv import load_dotenv
import os 
from services.whisper_utils import transcribe_audio


load_dotenv()

def get_llm_config():
  """
    Initiliazes the LLm with the Desired Configurations.

  """
  
  llm = ChatNVIDIA(
    model="meta/llama-3.3-70b-instruct",
    task='chat',
    temperature=0.6,
    top_p=0.7,
    max_tokens=4096,
  )
  return llm

async def llm_text_response(transcription_result:str):
    """
      This function takes the Transcription Result and Returns the Response from the LLM.

      Args: 
          transcription_result (str): The transcription result from the audio file. 

      Returns: 
          reponse (str) : The Response Generated by the LLM.
    """
    llm = get_llm_config()
    prompt = PromptTemplate(template = "You are a very helpful assistant and You will Explain the given {text} more elaborately",
    input_variables=["text"]                        
    )

    parser = StrOutputParser()
    llm_chain = prompt | llm | parser
    response  = await llm_chain.ainvoke({'text': transcription_result})
    return response